{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72d4301f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3376 images belonging to 4 classes.\n",
      "Found 841 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255.0,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2  # Split data into 80% training and 20% validation\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'eyedataset',\n",
    "    target_size=(224, 224),  # Adjust to the input size expected by ResNet-150\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',  # For multi-class classification\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    'eyedataset',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a048391a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "106/106 [==============================] - 958s 9s/step - loss: 1.4538 - accuracy: 0.2977 - val_loss: 1.3521 - val_accuracy: 0.3353\n",
      "Epoch 2/10\n",
      "106/106 [==============================] - 981s 9s/step - loss: 1.3612 - accuracy: 0.3282 - val_loss: 1.6627 - val_accuracy: 0.2545\n",
      "Epoch 3/10\n",
      "106/106 [==============================] - 932s 9s/step - loss: 1.3727 - accuracy: 0.3418 - val_loss: 1.3483 - val_accuracy: 0.2996\n",
      "Epoch 4/10\n",
      "106/106 [==============================] - 1788s 17s/step - loss: 1.3231 - accuracy: 0.3614 - val_loss: 1.3324 - val_accuracy: 0.3722\n",
      "Epoch 5/10\n",
      "106/106 [==============================] - 995s 9s/step - loss: 1.3166 - accuracy: 0.3697 - val_loss: 1.4321 - val_accuracy: 0.2949\n",
      "Epoch 6/10\n",
      "106/106 [==============================] - 982s 9s/step - loss: 1.3175 - accuracy: 0.3759 - val_loss: 1.3697 - val_accuracy: 0.3294\n",
      "Epoch 7/10\n",
      "106/106 [==============================] - 990s 9s/step - loss: 1.3178 - accuracy: 0.3711 - val_loss: 1.3363 - val_accuracy: 0.3389\n",
      "Epoch 8/10\n",
      "106/106 [==============================] - 992s 9s/step - loss: 1.3115 - accuracy: 0.3759 - val_loss: 1.3245 - val_accuracy: 0.3543\n",
      "Epoch 9/10\n",
      "106/106 [==============================] - 1000s 9s/step - loss: 1.3169 - accuracy: 0.3566 - val_loss: 1.3596 - val_accuracy: 0.3627\n",
      "Epoch 10/10\n",
      "106/106 [==============================] - 995s 9s/step - loss: 1.3014 - accuracy: 0.3883 - val_loss: 1.3445 - val_accuracy: 0.3472\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet152\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load pre-trained ResNet-152 model\n",
    "base_model = ResNet152(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Add custom classification head\n",
    "x = base_model.output\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(1024, activation='relu')(x)\n",
    "predictions = layers.Dense(len(train_generator.class_indices), activation='softmax')(x)\n",
    "\n",
    "# Create the model\n",
    "resnet_model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze base layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the ResNet model\n",
    "resnet_model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the ResNet model \n",
    "num_epochs = 10\n",
    "history_resnet = resnet_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=num_epochs,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=len(val_generator)\n",
    ")\n",
    "\n",
    "# Save the ResNet model\n",
    "resnet_model.save('resnet152_eye_disease.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89bfb339",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the trained ResNet model\n",
    "resnet_model = tf.keras.models.load_model('resnet152_eye_disease.h5')\n",
    "\n",
    "# Define a representative dataset for quantization (optional but recommended)\n",
    "def representative_data_gen():\n",
    "    for i in range(100):\n",
    "    \n",
    "            yield [tf.constant(train_generator[0][0][i:i+1])]\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32991320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4217 images belonging to 4 classes.\n",
      "1/1 [==============================] - 9s 9s/step\n",
      "Processed batch 1/50\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Processed batch 2/50\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "Processed batch 3/50\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Processed batch 4/50\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Processed batch 5/50\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "Processed batch 6/50\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Processed batch 7/50\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Processed batch 8/50\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Processed batch 9/50\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Processed batch 10/50\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Processed batch 11/50\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Processed batch 12/50\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Processed batch 13/50\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Processed batch 14/50\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Processed batch 15/50\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Processed batch 16/50\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Processed batch 17/50\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Processed batch 18/50\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "Processed batch 19/50\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Processed batch 20/50\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "Processed batch 21/50\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Processed batch 22/50\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "Processed batch 23/50\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Processed batch 24/50\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "Processed batch 25/50\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Processed batch 26/50\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Processed batch 27/50\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "Processed batch 28/50\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Processed batch 29/50\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "Processed batch 30/50\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Processed batch 31/50\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "Processed batch 32/50\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Processed batch 33/50\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "Processed batch 34/50\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Processed batch 35/50\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "Processed batch 36/50\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Processed batch 37/50\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "Processed batch 38/50\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "Processed batch 39/50\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "Processed batch 40/50\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Processed batch 41/50\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "Processed batch 42/50\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Processed batch 43/50\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "Processed batch 44/50\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Processed batch 45/50\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "Processed batch 46/50\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "Processed batch 47/50\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "Processed batch 48/50\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "Processed batch 49/50\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "Processed batch 50/50\n",
      "ResNet Model Accuracy: 0.38\n",
      "Average Inference Time (ResNet): 0.20747480794787407 seconds\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading  \n",
    "\n",
    "# Load the trained ResNet model\n",
    "resnet_model = tf.keras.models.load_model('resnet152_eye_disease.h5')\n",
    "\n",
    "# Define the validation data generator\n",
    "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "# Create a validation data generator\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    'eyedataset',  # Path to your validation data directory\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Initialize variables for accuracy and inference time\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "total_inference_time = 0\n",
    "\n",
    "# Set the number of batches or iterations you want to run\n",
    "num_batches_to_evaluate = 50  # Adjust as needed\n",
    "\n",
    "# Perform inference on the validation dataset for a specific number of batches\n",
    "for batch_idx, batch in enumerate(val_generator):\n",
    "    if batch_idx >= num_batches_to_evaluate:\n",
    "        break  # Exit the loop after the specified number of batches\n",
    "\n",
    "    images, labels = batch\n",
    "    start_time = time.time()\n",
    "    predictions = resnet_model.predict(images)\n",
    "    end_time = time.time()\n",
    "    inference_time = end_time - start_time\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    total_correct += np.sum(predicted_classes == np.argmax(labels, axis=1))\n",
    "    total_samples += len(images)\n",
    "    total_inference_time += inference_time\n",
    "\n",
    "    # Print progress\n",
    "    print(f'Processed batch {batch_idx + 1}/{num_batches_to_evaluate}')\n",
    "\n",
    "\n",
    "# Calculate accuracy and average inference time\n",
    "accuracy_resnet = total_correct / total_samples\n",
    "average_inference_time_resnet = total_inference_time / total_samples\n",
    "\n",
    "print(f'ResNet Model Accuracy: {accuracy_resnet}')\n",
    "print(f'Average Inference Time (ResNet): {average_inference_time_resnet} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df7f06f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 156). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\ASUS\\AppData\\Local\\Temp\\tmp7ta4iezw\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\ASUS\\AppData\\Local\\Temp\\tmp7ta4iezw\\assets\n",
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py:789: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
     ]
    }
   ],
   "source": [
    "# Convert to TFLite with static quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(resnet_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "quantized_tflite_model = converter.convert()\n",
    "\n",
    "# Save the quantized model\n",
    "with open(\"quantized_resnet152_static.tflite\", \"wb\") as f:\n",
    "    f.write(quantized_tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "030d314a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4217 images belonging to 4 classes.\n",
      "Processed batch 1/50\n",
      "Processed batch 2/50\n",
      "Processed batch 3/50\n",
      "Processed batch 4/50\n",
      "Processed batch 5/50\n",
      "Processed batch 6/50\n",
      "Processed batch 7/50\n",
      "Processed batch 8/50\n",
      "Processed batch 9/50\n",
      "Processed batch 10/50\n",
      "Processed batch 11/50\n",
      "Processed batch 12/50\n",
      "Processed batch 13/50\n",
      "Processed batch 14/50\n",
      "Processed batch 15/50\n",
      "Processed batch 16/50\n",
      "Processed batch 17/50\n",
      "Processed batch 18/50\n",
      "Processed batch 19/50\n",
      "Processed batch 20/50\n",
      "Processed batch 21/50\n",
      "Processed batch 22/50\n",
      "Processed batch 23/50\n",
      "Processed batch 24/50\n",
      "Processed batch 25/50\n",
      "Processed batch 26/50\n",
      "Processed batch 27/50\n",
      "Processed batch 28/50\n",
      "Processed batch 29/50\n",
      "Processed batch 30/50\n",
      "Processed batch 31/50\n",
      "Processed batch 32/50\n",
      "Processed batch 33/50\n",
      "Processed batch 34/50\n",
      "Processed batch 35/50\n",
      "Processed batch 36/50\n",
      "Processed batch 37/50\n",
      "Processed batch 38/50\n",
      "Processed batch 39/50\n",
      "Processed batch 40/50\n",
      "Processed batch 41/50\n",
      "Processed batch 42/50\n",
      "Processed batch 43/50\n",
      "Processed batch 44/50\n",
      "Processed batch 45/50\n",
      "Processed batch 46/50\n",
      "Processed batch 47/50\n",
      "Processed batch 48/50\n",
      "Processed batch 49/50\n",
      "Processed batch 50/50\n",
      "Quantized Model Accuracy (Static): 0.2525\n",
      "Average Inference Time (Quantized Static): 0.008245354890823365 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load the quantized TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path='quantized_resnet152_static.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "batch_size=32\n",
    "# Define the validation data generator\n",
    "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "# Create a validation data generator\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    'eyedataset',  # Path to your validation data directory\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,  # Set the batch size\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Initialize variables for accuracy and inference time\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "total_inference_time = 0\n",
    "\n",
    "# Set the number of batches you want to evaluate (adjust as needed)\n",
    "num_batches_to_evaluate = 50\n",
    "\n",
    "# Perform inference on batches of images\n",
    "for batch_index in range(num_batches_to_evaluate):\n",
    "    images, labels = val_generator.next()  # Get the next batch of images and labels\n",
    "\n",
    "    # Set the batch size to 1\n",
    "    images = images[:1]\n",
    "\n",
    "    # Prepare the input tensor\n",
    "    interpreter.set_tensor(input_details[0]['index'], images)\n",
    "\n",
    "    start_time = time.time()\n",
    "    interpreter.invoke()\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    end_time = time.time()\n",
    "\n",
    "    inference_time = end_time - start_time\n",
    "    predicted_classes = np.argmax(output_data, axis=1)\n",
    "\n",
    "    # Compare predicted classes with true labels\n",
    "    correct_predictions = np.sum(predicted_classes == np.argmax(labels, axis=1))\n",
    "    total_correct += correct_predictions\n",
    "    total_samples += batch_size\n",
    "    total_inference_time += inference_time\n",
    "\n",
    "    # Print progress\n",
    "    print(f'Processed batch {batch_index + 1}/{num_batches_to_evaluate}')\n",
    "\n",
    "# Calculate accuracy and average inference time\n",
    "accuracy_quantized = total_correct / total_samples\n",
    "average_inference_time_quantized = total_inference_time / total_samples\n",
    "\n",
    "print(f'Quantized Model Accuracy (Static): {accuracy_quantized}')\n",
    "print(f'Average Inference Time (Quantized Static): {average_inference_time_quantized} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215cc5c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
